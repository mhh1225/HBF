import sys
import os
import re
import json
from typing import Dict, Any, List

# --- è·¯å¾„é»‘é­”æ³•ï¼šç¡®ä¿èƒ½å¼•ç”¨åˆ° InsightEngine ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)

# å¼•ç”¨é¡¹ç›®å·¥å…·
from InsightEngine.tools.search import MediaCrawlerDB
import requests


class LinkRepairAgent:
    """
    é“¾æ¥ä¿®å¤ç‰¹å·¥ v2.0 (å¢å¼ºç‰ˆ)ï¼šä¿®å¤ KeyError bug å¹¶ä¼˜åŒ–æœç´¢åŒ¹é…
    """

    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ–é“¾æ¥ä¿®å¤ç‰¹å·¥...")
        try:
            self.db_tool = MediaCrawlerDB()
            print("âœ… å·²è¿æ¥èˆ†æƒ…æ•°æ®åº“ (MediaCrawlerDB)")
        except Exception as e:
            print(f"âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            self.db_tool = None

    def _is_url_alive(self, url: str) -> bool:
        """æ£€æµ‹é“¾æ¥æ˜¯å¦å­˜æ´»"""
        if not url or len(url) < 10 or 'http' not in url:
            return False
        # è¿‡æ»¤æ‰æ˜æ˜¾çš„å‡é“¾æ¥/æˆªæ–­é“¾æ¥
        if '...' in url or 'example.com' in url:
            return False

        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            r = requests.head(url, headers=headers, timeout=2)
            if r.status_code < 400: return True
            r = requests.get(url, headers=headers, timeout=3, stream=True)
            if r.status_code < 400: return True
        except:
            return False
        return False

    def _find_real_url_from_db(self, query_text: str) -> str:
        """ä»æ•°æ®åº“åæŸ¥çœŸå®é“¾æ¥"""
        if not self.db_tool or not query_text:
            return None

        # 1. ä¼˜åŒ–å…³é”®è¯æå–ï¼šä¿ç•™ç©ºæ ¼ï¼Œé¿å…æŠŠ "2025 æ ¡åº†" å˜æˆ "2025æ ¡åº†" å¯¼è‡´æœç´¢å˜å·®
        # åªä¿ç•™æ±‰å­—ã€å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼
        clean_query = re.sub(r'[^\w\u4e00-\u9fa5\s]', ' ', query_text)
        # å»æ‰å¤šä½™ç©ºæ ¼å¹¶æˆªå–å‰20ä¸ªå­—ç¬¦ï¼ˆå¤ªé•¿æœä¸åˆ°ï¼‰
        clean_query = " ".join(clean_query.split())[:20]

        if len(clean_query) < 2: return None

        print(f"   ğŸ” æ­£åœ¨åº“ä¸­é‡æœçº¿ç´¢: '{clean_query}'...", end="")

        try:
            # é™åˆ¶è¿”å›1æ¡
            response = self.db_tool.search_topic_globally(topic=clean_query, limit_per_table=1)
            if response.results and len(response.results) > 0:
                candidate = response.results[0]
                if candidate.url and "http" in candidate.url:
                    print(f" [âœ… æ‰¾åˆ°: {candidate.url[:30]}...]")
                    return candidate.url
        except Exception as e:
            print(f" [æœç´¢å‡ºé”™: {e}]")

        print(" [âŒ æœªæ‰¾åˆ°]")
        return None

    def repair_process(self, report_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸»æµç¨‹ï¼šå®‰å…¨éå†å¹¶ä¿®å¤"""
        fixed_count = 0
        print("ğŸš€ å¼€å§‹æ‰§è¡ŒæŠ¥å‘Šæ·±åº¦ä¿®å¤...")

        def process_blocks_recursive(blocks):
            nonlocal fixed_count
            for block in blocks:
                # å¤„ç†æ®µè½
                if block.get('type') == 'paragraph':
                    inlines = block.get('inlines', [])
                    for run in inlines:
                        marks = run.get('marks', [])
                        # ä½¿ç”¨ while å¾ªç¯ä»¥ä¾¿å®‰å…¨åˆ é™¤å…ƒç´ ï¼ˆè™½ç„¶è¿™é‡Œæˆ‘ä»¬ä¸»è¦æ˜¯ä¿®æ”¹ï¼‰
                        for mark in marks:
                            if mark.get('type') == 'link':
                                # --- 1. å®‰å…¨è·å– URL ---
                                attrs = mark.get('attrs', {})
                                original_url = attrs.get('href', '')
                                anchor_text = run.get('text', '')

                                # --- 2. åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®å¤ ---
                                if not self._is_url_alive(original_url):
                                    # print(f"ğŸ’€ å‘ç°æ­»é“¾: {original_url}")

                                    # --- 3. å°è¯•æœç´¢çœŸé“¾æ¥ ---
                                    real_url = self._find_real_url_from_db(anchor_text)

                                    if real_url:
                                        # --- 4. ã€å…³é”®ä¿®å¤ã€‘å®‰å…¨èµ‹å€¼ ---
                                        # å¦‚æœ 'attrs' ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»ºå®ƒï¼Œé˜²æ­¢ KeyError
                                        if 'attrs' not in mark:
                                            mark['attrs'] = {}

                                        mark['attrs']['href'] = real_url

                                        # å¯é€‰ï¼šåœ¨æ–‡æœ¬ååŠ ä¸ªæ ‡è®°è¯æ˜ä¿®è¿‡äº†
                                        # run['text'] += " [é“¾æ¥å·²ä¿®å¤]"
                                        fixed_count += 1
                                    else:
                                        # æ²¡æ•‘å›æ¥ï¼Œä¸ºäº†ä¸è®©ç”¨æˆ·ç‚¹è¿›å»æŠ¥é”™ï¼ŒæŒ‡å‘ç©ºæˆ–ç§»é™¤
                                        if 'attrs' not in mark: mark['attrs'] = {}
                                        mark['attrs']['href'] = "javascript:void(0);"  # ç‚¹å‡»æ— ååº”
                                        if "(æ¥æºæ— æ³•è®¿é—®)" not in run['text']:
                                            run['text'] += " (æ¥æºæš‚ä¸å¯ç”¨)"

                # é€’å½’å­ç»“æ„
                if 'items' in block:  # åˆ—è¡¨
                    for item in block['items']: process_blocks_recursive(item)
                if 'blocks' in block:  # å¼•ç”¨å—
                    process_blocks_recursive(block['blocks'])
                if 'rows' in block:  # è¡¨æ ¼
                    for row in block['rows']:
                        for cell in row.get('cells', []):
                            if 'blocks' in cell: process_blocks_recursive(cell['blocks'])

        if 'chapters' in report_data:
            for chapter in report_data['chapters']:
                if 'blocks' in chapter:
                    process_blocks_recursive(chapter['blocks'])

        print(f"âœ¨ ä¿®å¤å®Œæˆï¼æˆåŠŸæŒ½æ•‘äº† {fixed_count} ä¸ªé“¾æ¥ã€‚")
        return report_data


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    input_path = "../../logs/report_baseline.json"
    if os.path.exists(input_path):
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        agent = LinkRepairAgent()
        fixed = agent.repair_process(data)
        print("æµ‹è¯•å®Œæˆ")
    else:
        print("è¯·åœ¨ ReportEngine ç›®å½•ä¸‹è¿è¡Œæˆ–è°ƒæ•´æµ‹è¯•è·¯å¾„")